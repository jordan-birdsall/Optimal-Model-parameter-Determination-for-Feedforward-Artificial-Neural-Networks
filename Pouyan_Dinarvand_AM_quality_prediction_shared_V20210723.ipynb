{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pouyan_Dinarvand_AM_quality_prediction_shared.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gj4081U1Dne"
      },
      "source": [
        "'''\n",
        "All Rights Reserved - Pouyan Dinarvand\n",
        "date: 23/07/2021\n",
        "-> Performing ML classification (for predicting Good/Bad quality) algorithms on AM dataset \n",
        "'''\n",
        "\n",
        "# import libraries\n",
        "!pip install hvplot\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import panel as pn\n",
        "from panel.interact import interact\n",
        "\n",
        "import hvplot.pandas \n",
        "import holoviews as hv\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.tree import export_text\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from joblib import dump, load\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# read data\n",
        "df = pd.read_csv('AM_Ti6Al4V_process_property_parameters_one_Hot_encoded_relative_density.csv')\n",
        "\n",
        "\n",
        "#@title ML Classification - Model Training {run:'auto'}\n",
        "\n",
        "#@markdown >  Classification\n",
        "algorithm = \"KNN\" #@param [\"NeuralNetwork\",\"KNN\", \"Tree\",\"RandomForest\", \"AdaBoost\", \"GradientBoosting\", \"GaussianNB\", \"GaussianProcess\", \"SVM\"]\n",
        "test_train_ratio = 0.3 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "scale_train_test = True #@param {type:\"boolean\"}\n",
        "\n",
        "use_PCA = False #@param {type:\"boolean\"}\n",
        "include_machine_type_and_layer_tickness = False #@param {type:\"boolean\"}\n",
        "threshold_good_bad_quality_AM = 99.5 #@param {type:\"slider\", min:0, max:100, step:0.1}\n",
        "print_params = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "#@markdown >  Optimization\n",
        "cv =  2 #@param {type:\"integer\"}\n",
        "only_print_results_best_model = True #@param {type:\"boolean\"}\n",
        "#@markdown >  Saving ML Model\n",
        "save_model = True #@param {type:\"boolean\"}\n",
        "\n",
        "if(print_params):\n",
        "    print('*************** Classification Parameters ****************')\n",
        "    print('algorithm = ', algorithm)\n",
        "    print('test_train_ratio = ', test_train_ratio)\n",
        "    print('scale_train_test = ', scale_train_test)\n",
        "    print('use_PCA = ', use_PCA)\n",
        "    print('include_machine_type_and_layer_tickness = ', include_machine_type_and_layer_tickness)\n",
        "    print('threshold_good_bad_quality_AM = ', threshold_good_bad_quality_AM)\n",
        "\n",
        "    print('*************** Optimization Parameters ***************')\n",
        "    print('cv = ', cv)\n",
        "    print('save_model = ', save_model)\n",
        "\n",
        "\n",
        "######################################### Feature Selection #########################################\n",
        "# get data\n",
        "if(include_machine_type_and_layer_tickness):\n",
        "    feature_names_list = ['Laser_Power', 'Laser_Speed', 'Hatch_Spacing', 'Layer_Thickness', 'Powder_Size', 'Laser_Energy', \n",
        "                          'Machine_Type_Concept Laser M2', 'Machine_Type_Concept laser M',\n",
        "                          'Machine_Type_Concept laser M3', 'Machine_Type_EOS M270',\n",
        "                          'Machine_Type_SLM', 'Machine_Type_SLM 125 HL',\n",
        "                          'Machine_Type_SLM 250 HL', 'Machine_Type_SLM 280 HL',\n",
        "                          'Machine_Type_Self-developed SLM', 'Machine_Type_Trumpf LF250']\n",
        "\n",
        "\n",
        "else:\n",
        "    feature_names_list = ['Laser_Power', 'Laser_Speed', 'Hatch_Spacing', 'Powder_Size', 'Laser_Energy']\n",
        "\n",
        "\n",
        "X = df.copy()[feature_names_list].values\n",
        "\n",
        "#binarize target variable for classification\n",
        "y = np.where(df.copy()['Relative_Density'].values >= threshold_good_bad_quality_AM, 1, 0) # 1 = Good quality & 0 = bad quality\n",
        "\n",
        "\n",
        "'''\n",
        "if(print_params):\n",
        "    print('number of good quality (label 1) samples = ', sum(y))\n",
        "    print('number of bad quality (label 0) samples = ', len(y) - sum(y))\n",
        "    print('Ratio ones (good quality parts) in the dataset = ', sum(np.copy(y)/len(y)))\n",
        "'''\n",
        "\n",
        "#print(X)\n",
        "#print(y)\n",
        "\n",
        "# compute mutual information between features and target variable\n",
        "if(print_params):\n",
        "\n",
        "    print('*********************  Mutual Information *********************')\n",
        "    MI_df = pd.DataFrame({'Features': feature_names_list, \n",
        "                          'MI': mutual_info_classif(X = np.copy(X), y = np.copy(y))\n",
        "                         })\n",
        "    print(MI_df)\n",
        "    print('****************************************************************')\n",
        "\n",
        "\n",
        "\n",
        "#spliting data into train & test sets for classification\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.copy(X), np.copy(y), test_size= test_train_ratio, random_state = 123)\n",
        "\n",
        "# initialize ML pipleline\n",
        "pipeline_steps = []\n",
        "\n",
        "# scaling data\n",
        "if(scale_train_test):\n",
        "    pipeline_steps.append(('scaler', StandardScaler()))\n",
        "    print('StandardScaler added to the pipeline.')\n",
        "\n",
        "# Dimensionality Reduction via PCA\n",
        "if(use_PCA):\n",
        "    n_components_pca = 3 \n",
        "    feature_names_list = [ 'PC'+str(i+1) for i in np.arange(0,n_components_pca)]\n",
        "    pipeline_steps.append(('pca', PCA(n_components = n_components_pca)))\n",
        "    print('PCA with n_components = ' + str(n_components_pca) + ' added to the pipeline.')\n",
        "\n",
        "if(print_params):\n",
        "    print('Name of features:')\n",
        "    print(feature_names_list)\n",
        "\n",
        "    '''\n",
        "    print('X_train :', X_train, 'X_test :', X_test)\n",
        "    print('y_train :', y_train,'y_test :',  y_test)\n",
        "    '''\n",
        "    print('X_train shape :', X_train.shape, 'X_test shape :', X_test.shape)\n",
        "    print('y_train shape :', y_train.shape, 'y_test shape :',  y_test.shape)\n",
        "\n",
        "############################# Model training ###################################\n",
        "\n",
        "# KNN\n",
        "if(algorithm == 'KNN'):\n",
        "    \n",
        "    param_grid = [{'model__n_neighbors' : np.arange(1,25),\n",
        "                   'model__p': [1,2,3,5,10,100],\n",
        "                   'model__weights': ['uniform', 'distance'],\n",
        "                   }] # [{exp1},{exp2}]\n",
        "               \n",
        "        \n",
        "    pipeline_steps.append(('model', KNeighborsClassifier()))\n",
        "    \n",
        "\n",
        "# NeuralNetwork\n",
        "elif(algorithm == 'NeuralNetwork'):\n",
        "\n",
        "    '''\n",
        "    param_grid = [{'model__activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
        "                    'model__solver': ['lbfgs', 'sgd', 'adam'],\n",
        "                    'model__alpha': [0.001,0.01,0.1,1,10],\n",
        "                    'model__hidden_layer_sizes': [ (2,) ,(5,), (10,), (100,), (2,2), (5,5), (10,10), (2,2,2,2,2), (5,5,5,5,5), (10,10,10)],\n",
        "                    'model__learning_rate_init': [0.0001,0.01,0.1,1],\n",
        "                    'model__max_iter': [50,500],\n",
        "                    'model__learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
        "                    'model__early_stopping': [True, False],\n",
        "                  }]\n",
        "    '''\n",
        "    param_grid = [{'model__activation' : ['tanh'],\n",
        "                    'model__solver': ['lbfgs'],\n",
        "                    'model__alpha': [0.001,0.01,0.1,1,10],\n",
        "                    'model__hidden_layer_sizes': [ (100,), (100,100),  (100,100,100)],\n",
        "                    'model__learning_rate_init': [0.0001,0.01,0.1,1],\n",
        "                    'model__max_iter': [50,500],\n",
        "                    'model__learning_rate' : ['constant'],\n",
        "                    'model__early_stopping': [True],\n",
        "                  }]\n",
        "\n",
        "    pipeline_steps.append(('model', MLPClassifier(verbose = False, random_state = 123)))\n",
        "\n",
        "\n",
        "\n",
        "# Tree \n",
        "elif(algorithm == 'Tree'):\n",
        "    \n",
        "    param_grid = [{'model__max_depth' : [None, 2,5, 10, 25,50,100],\n",
        "                   'model__criterion': ['entropy', 'gini'],\n",
        "                   'model__splitter' : ['best', 'random'],\n",
        "                   }] # [{exp1},{exp2}]\n",
        "        \n",
        "    pipeline_steps.append(('model', tree.DecisionTreeClassifier(random_state = 123)))\n",
        "    \n",
        "\n",
        "# RandomForest\n",
        "elif(algorithm == 'RandomForest'):\n",
        "\n",
        "    '''\n",
        "    param_grid = [{'model__n_estimators' :[100,1000,5000],\n",
        "                   'model__criterion': ['entropy', 'gini'],\n",
        "                   'model__max_depth': [None, 2, 5, 10, 25, 50, 100],\n",
        "                  }]\n",
        "    '''\n",
        "    param_grid = [{'model__n_estimators' :[5000],\n",
        "                   'model__criterion': ['entropy'],\n",
        "                   'model__max_depth': [None],\n",
        "                  }]\n",
        "    \n",
        "    pipeline_steps.append(('model', RandomForestClassifier(random_state = 123)))\n",
        "    \n",
        "\n",
        "# AdaBoost\n",
        "elif(algorithm == 'AdaBoost'):\n",
        "\n",
        "    param_grid = [{'model__n_estimators' :[100,1000,5000],\n",
        "                   'model__learning_rate': [0.0001, 0.01,0.1,1,10],\n",
        "                  }]\n",
        "    pipeline_steps.append(('model', AdaBoostClassifier(random_state = 123)))\n",
        "    \n",
        "\n",
        "# GradientBoosting\n",
        "elif(algorithm == 'GradientBoosting'):\n",
        "\n",
        "    param_grid = [{'model__n_estimators' :[100, 1000,5000],\n",
        "                   'model__learning_rate': [0.0001, 0.01,0.1,1,10],\n",
        "                   'model__max_depth': [None, 2, 5, 10, 25, 50, 100],\n",
        "                   'model__criterion': ['friedman_mse', 'mse', 'mae'],\n",
        "                  }]\n",
        "    pipeline_steps.append(('model', GradientBoostingClassifier(random_state = 123)))\n",
        "    \n",
        "\n",
        "# GaussianNB\n",
        "elif(algorithm == 'GaussianNB'):\n",
        "    \n",
        "    param_grid = [{\n",
        "                  }]\n",
        "    pipeline_steps.append(('model', GaussianNB()))\n",
        "    \n",
        "\n",
        "# GaussianProcess\n",
        "elif(algorithm == 'GaussianProcess'):\n",
        "    \n",
        "    param_grid = [{'model__kernel' :[1 * RBF(10), 0.1 * RBF(10), 0.001 * RBF(10) ,1 * RBF(1), 0.1 * RBF(1), 0.001 * RBF(1), 1 * RBF(0.01), 0.1 * RBF(0.01), 0.001 * RBF(0.01),],\n",
        "                  }]\n",
        "    pipeline_steps.append(('model', GaussianProcessClassifier(random_state = 123)))\n",
        "    \n",
        "\n",
        "# SVM\n",
        "elif(algorithm == 'SVM'):\n",
        "    \n",
        "    param_grid = [{'model__C' :[0.0001,0.001,0.01,0.1,1,10,100,1000],\n",
        "                   'model__gamma' :['scale', 'auto'],\n",
        "                   'model__kernel' :['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "                  }]\n",
        "    pipeline_steps.append(('model', SVC(random_state = 123, verbose = False))) # set prabability =True to get prob estimates\n",
        "    \n",
        "\n",
        "#########################################  Fit Pipeline #########################################\n",
        "print(algorithm + ' added to the pipeline.')\n",
        "my_pipeline = Pipeline(pipeline_steps)\n",
        "\n",
        "print('****************************************************************')\n",
        "\n",
        "print('param_grid:')\n",
        "print(param_grid)\n",
        "\n",
        "print('Optimization in process ... ')\n",
        "\n",
        "model = GridSearchCV(estimator = my_pipeline, param_grid = param_grid, n_jobs = -1, cv = cv).fit(X_train, y_train) \n",
        "        \n",
        "if(print_params):\n",
        "    print('Best hyper-parameters found in the hyper-parameters grid:' )\n",
        "    print(model.best_params_)\n",
        "\n",
        "    print('~~~~~~~~~~~~~~~~~~~~ Best Algorithm hyper-parameters ~~~~~~~~~~~~~~~~~~~~')\n",
        "    print(model.best_estimator_.get_params)\n",
        "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "            \n",
        "    print('Optimization cv results:')\n",
        "    for i, j, k in zip(model.cv_results_['params'], model.cv_results_['std_test_score'], model.cv_results_['mean_test_score']): \n",
        "        if( only_print_results_best_model):# only print the best cv score\n",
        "            if(i == model.best_params_): \n",
        "                print('- params: ', i)\n",
        "                print('std cv score = ', np.round(j,2) , ' , mean cv score = ', np.round(k,2) )\n",
        "                break        \n",
        "        else:\n",
        "            print('- params: ', i)\n",
        "            print('std cv score = ', np.round(j,2) , ' , mean cv score = ', np.round(k,2) )\n",
        "\n",
        "    # specific prints for tree-based ML models\n",
        "    if(algorithm == 'Tree' or algorithm == 'GradientBoosting' or algorithm == 'AdaBoost' or algorithm == 'RandomForest'):\n",
        "        if(algorithm == 'Tree'):\n",
        "            text_tree = export_text(model.best_estimator_['model'], feature_names = feature_names_list)\n",
        "            print('Best Tree Text: ')\n",
        "            print(text_tree)\n",
        "\n",
        "        print('***** Feature Importances *****')\n",
        "        ft_imp = pd.DataFrame({'Features': feature_names_list, \n",
        "                               'Importance': model.best_estimator_['model'].feature_importances_})\n",
        "        print(ft_imp)\n",
        "\n",
        "######################################### Prediction on test set #########################################\n",
        "#y_pred_train = model.predict(np.copy(X_train))\n",
        "y_pred_test = model.predict(np.copy(X_test))\n",
        "\n",
        "\n",
        "'''\n",
        "print( 'accuracy_score on train set = ', accuracy_score(y_true = np.copy(y_train), y_pred = np.copy(y_pred_train)) )\n",
        "print( 'accuracy_score on test set = ', accuracy_score(y_true = np.copy(y_test), y_pred = np.copy(y_pred_test)) )\n",
        "'''\n",
        "print('****************************************************************')\n",
        "\n",
        "'''\n",
        "print('Classification Report on the train set:')\n",
        "print(classification_report(y_true = np.copy(y_train), y_pred = np.copy(y_pred_train), labels=[0, 1]))\n",
        "'''\n",
        "\n",
        "print('Classification Report on the test set:')\n",
        "print(classification_report(y_true = np.copy(y_test), y_pred = np.copy(y_pred_test), labels=[0, 1]))\n",
        "\n",
        "print('Confusion Matrix')\n",
        "cm = confusion_matrix(y_true = np.copy(y_test), y_pred = np.copy(y_pred_test), labels=[0, 1])\n",
        "cm_plot = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = [0, 1])\n",
        "cm_plot.plot()\n",
        "\n",
        "########################################## Save Model (pickle) #########################################\n",
        "if(save_model):\n",
        "    dump(model.best_estimator_, 'model.joblib')  \n",
        "    print('model saved as model.joblib')\n",
        "\n",
        "########################################## Clear Memory #########################################\n",
        "try:\n",
        "    del my_pipeline, model, X, y, param_grid\n",
        "except Exception as e:\n",
        "    pass    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}